{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5eBZfrXkm5i"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade --force-reinstall youtube-transcript-api langchain-community langchain-google-genai faiss-cpu tiktoken python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r0ZHDLpdlVs5",
        "outputId": "13e2e067-12f2-443e-d8a0-8e232295b95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Using cached youtube_transcript_api-1.2.4-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langchain-community\n",
            "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-4.2.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting python-dotenv\n",
            "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting defusedxml<0.8.0,>=0.7.1 (from youtube-transcript-api)\n",
            "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting requests (from youtube-transcript-api)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n",
            "  Using cached langchain_core-1.2.14-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Using cached langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n",
            "  Using cached sqlalchemy-2.0.46-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting PyYAML<7.0.0,>=5.3.0 (from langchain-community)\n",
            "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Using cached aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-community)\n",
            "  Using cached tenacity-9.1.4-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
            "  Using cached pydantic_settings-2.13.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting langsmith<1.0.0,>=0.1.125 (from langchain-community)\n",
            "  Using cached langsmith-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Using cached httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting numpy>=1.26.2 (from langchain-community)\n",
            "  Using cached numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-genai<2.0.0,>=1.56.0 (from langchain-google-genai)\n",
            "  Using cached google_genai-1.64.0-py3-none-any.whl.metadata (53 kB)\n",
            "Collecting pydantic<3.0.0,>=2.0.0 (from langchain-google-genai)\n",
            "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
            "Collecting packaging (from faiss-cpu)\n",
            "  Using cached packaging-26.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting regex>=2022.1.18 (from tiktoken)\n",
            "  Using cached regex-2026.2.19-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached multidict-6.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Using cached marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting anyio<5.0.0,>=4.8.0 (from google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting google-auth<3.0.0,>=2.47.0 (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached google_auth-2.48.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting httpx<1.0.0,>=0.28.1 (from google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting websockets<15.1.0,>=13.0.0 (from google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-extensions<5.0.0,>=4.11.0 (from google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting distro<2,>=1.7.0 (from google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting sniffio (from google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Using cached langchain_text_splitters-1.1.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.1->langchain-community)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.0.1->langchain-community)\n",
            "  Using cached uuid_utils-0.14.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
            "  Using cached orjson-3.11.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting xxhash>=3.0.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
            "  Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
            "  Using cached zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.0.0->langchain-google-genai)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.0.0->langchain-google-genai)\n",
            "  Using cached pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.0.0->langchain-google-genai)\n",
            "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->youtube-transcript-api)\n",
            "  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->youtube-transcript-api)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->youtube-transcript-api)\n",
            "  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->youtube-transcript-api)\n",
            "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n",
            "  Using cached greenlet-3.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting cryptography>=38.0.3 (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached cryptography-46.0.5-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting httpcore==1.* (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community)\n",
            "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting cffi>=2.0.0 (from cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached pyasn1-0.6.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pycparser (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Using cached pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Using cached youtube_transcript_api-1.2.4-py3-none-any.whl (485 kB)\n",
            "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "Using cached langchain_google_genai-4.2.1-py3-none-any.whl (66 kB)\n",
            "Using cached faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Using cached aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Using cached google_genai-1.64.0-py3-none-any.whl (728 kB)\n",
            "Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
            "Using cached langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_core-1.2.14-py3-none-any.whl (501 kB)\n",
            "Using cached langsmith-0.7.5-py3-none-any.whl (324 kB)\n",
            "Using cached numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "Using cached packaging-26.0-py3-none-any.whl (74 kB)\n",
            "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
            "Using cached pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Using cached pydantic_settings-2.13.1-py3-none-any.whl (58 kB)\n",
            "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
            "Using cached regex-2026.2.19-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached sqlalchemy-2.0.46-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "Using cached tenacity-9.1.4-py3-none-any.whl (28 kB)\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached anyio-4.12.1-py3-none-any.whl (113 kB)\n",
            "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
            "Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
            "Using cached google_auth-2.48.0-py3-none-any.whl (236 kB)\n",
            "Using cached greenlet-3.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (609 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached langchain_text_splitters-1.1.1-py3-none-any.whl (35 kB)\n",
            "Using cached marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "Using cached multidict-6.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
            "Using cached orjson-3.11.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
            "Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
            "Using cached uuid_utils-0.14.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
            "Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
            "Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
            "Using cached zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached cryptography-46.0.5-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
            "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Using cached cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (219 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
            "Using cached pycparser-3.0-py3-none-any.whl (48 kB)\n",
            "Installing collected packages: filetype, zstandard, xxhash, websockets, uuid-utils, urllib3, typing-extensions, tenacity, sniffio, regex, PyYAML, python-dotenv, pycparser, pyasn1, propcache, packaging, orjson, numpy, mypy-extensions, multidict, jsonpointer, idna, httpx-sse, h11, greenlet, frozenlist, distro, defusedxml, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspection, typing-inspect, SQLAlchemy, rsa, requests, pydantic-core, pyasn1-modules, marshmallow, jsonpatch, httpcore, faiss-cpu, cffi, anyio, aiosignal, youtube-transcript-api, tiktoken, requests-toolbelt, pydantic, httpx, dataclasses-json, cryptography, aiohttp, pydantic-settings, langsmith, google-auth, langchain-core, langchain-text-splitters, google-genai, langchain-google-genai, langchain-classic, langchain-community\n",
            "  Attempting uninstall: filetype\n",
            "    Found existing installation: filetype 1.2.0\n",
            "    Uninstalling filetype-1.2.0:\n",
            "      Successfully uninstalled filetype-1.2.0\n",
            "  Attempting uninstall: zstandard\n",
            "    Found existing installation: zstandard 0.25.0\n",
            "    Uninstalling zstandard-0.25.0:\n",
            "      Successfully uninstalled zstandard-0.25.0\n",
            "  Attempting uninstall: xxhash\n",
            "    Found existing installation: xxhash 3.6.0\n",
            "    Uninstalling xxhash-3.6.0:\n",
            "      Successfully uninstalled xxhash-3.6.0\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: uuid-utils\n",
            "    Found existing installation: uuid_utils 0.14.0\n",
            "    Uninstalling uuid_utils-0.14.0:\n",
            "      Successfully uninstalled uuid_utils-0.14.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.6.3\n",
            "    Uninstalling urllib3-2.6.3:\n",
            "      Successfully uninstalled urllib3-2.6.3\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.1.4\n",
            "    Uninstalling tenacity-9.1.4:\n",
            "      Successfully uninstalled tenacity-9.1.4\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.1\n",
            "    Uninstalling sniffio-1.3.1:\n",
            "      Successfully uninstalled sniffio-1.3.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2026.2.19\n",
            "    Uninstalling regex-2026.2.19:\n",
            "      Successfully uninstalled regex-2026.2.19\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.3\n",
            "    Uninstalling PyYAML-6.0.3:\n",
            "      Successfully uninstalled PyYAML-6.0.3\n",
            "  Attempting uninstall: python-dotenv\n",
            "    Found existing installation: python-dotenv 1.2.1\n",
            "    Uninstalling python-dotenv-1.2.1:\n",
            "      Successfully uninstalled python-dotenv-1.2.1\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 3.0\n",
            "    Uninstalling pycparser-3.0:\n",
            "      Successfully uninstalled pycparser-3.0\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.6.2\n",
            "    Uninstalling pyasn1-0.6.2:\n",
            "      Successfully uninstalled pyasn1-0.6.2\n",
            "  Attempting uninstall: propcache\n",
            "    Found existing installation: propcache 0.4.1\n",
            "    Uninstalling propcache-0.4.1:\n",
            "      Successfully uninstalled propcache-0.4.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 26.0\n",
            "    Uninstalling packaging-26.0:\n",
            "      Successfully uninstalled packaging-26.0\n",
            "  Attempting uninstall: orjson\n",
            "    Found existing installation: orjson 3.11.7\n",
            "    Uninstalling orjson-3.11.7:\n",
            "      Successfully uninstalled orjson-3.11.7\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.4.2\n",
            "    Uninstalling numpy-2.4.2:\n",
            "      Successfully uninstalled numpy-2.4.2\n",
            "  Attempting uninstall: mypy-extensions\n",
            "    Found existing installation: mypy_extensions 1.1.0\n",
            "    Uninstalling mypy_extensions-1.1.0:\n",
            "      Successfully uninstalled mypy_extensions-1.1.0\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.7.1\n",
            "    Uninstalling multidict-6.7.1:\n",
            "      Successfully uninstalled multidict-6.7.1\n",
            "  Attempting uninstall: jsonpointer\n",
            "    Found existing installation: jsonpointer 3.0.0\n",
            "    Uninstalling jsonpointer-3.0.0:\n",
            "      Successfully uninstalled jsonpointer-3.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "  Attempting uninstall: httpx-sse\n",
            "    Found existing installation: httpx-sse 0.4.3\n",
            "    Uninstalling httpx-sse-0.4.3:\n",
            "      Successfully uninstalled httpx-sse-0.4.3\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 3.3.1\n",
            "    Uninstalling greenlet-3.3.1:\n",
            "      Successfully uninstalled greenlet-3.3.1\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.8.0\n",
            "    Uninstalling frozenlist-1.8.0:\n",
            "      Successfully uninstalled frozenlist-1.8.0\n",
            "  Attempting uninstall: distro\n",
            "    Found existing installation: distro 1.9.0\n",
            "    Uninstalling distro-1.9.0:\n",
            "      Successfully uninstalled distro-1.9.0\n",
            "  Attempting uninstall: defusedxml\n",
            "    Found existing installation: defusedxml 0.7.1\n",
            "    Uninstalling defusedxml-0.7.1:\n",
            "      Successfully uninstalled defusedxml-0.7.1\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.4\n",
            "    Uninstalling charset-normalizer-3.4.4:\n",
            "      Successfully uninstalled charset-normalizer-3.4.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2026.1.4\n",
            "    Uninstalling certifi-2026.1.4:\n",
            "      Successfully uninstalled certifi-2026.1.4\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.7.0\n",
            "    Uninstalling annotated-types-0.7.0:\n",
            "      Successfully uninstalled annotated-types-0.7.0\n",
            "  Attempting uninstall: aiohappyeyeballs\n",
            "    Found existing installation: aiohappyeyeballs 2.6.1\n",
            "    Uninstalling aiohappyeyeballs-2.6.1:\n",
            "      Successfully uninstalled aiohappyeyeballs-2.6.1\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.22.0\n",
            "    Uninstalling yarl-1.22.0:\n",
            "      Successfully uninstalled yarl-1.22.0\n",
            "  Attempting uninstall: typing-inspection\n",
            "    Found existing installation: typing-inspection 0.4.2\n",
            "    Uninstalling typing-inspection-0.4.2:\n",
            "      Successfully uninstalled typing-inspection-0.4.2\n",
            "  Attempting uninstall: typing-inspect\n",
            "    Found existing installation: typing-inspect 0.9.0\n",
            "    Uninstalling typing-inspect-0.9.0:\n",
            "      Successfully uninstalled typing-inspect-0.9.0\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.46\n",
            "    Uninstalling SQLAlchemy-2.0.46:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.46\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9.1\n",
            "    Uninstalling rsa-4.9.1:\n",
            "      Successfully uninstalled rsa-4.9.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.5\n",
            "    Uninstalling requests-2.32.5:\n",
            "      Successfully uninstalled requests-2.32.5\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.41.5\n",
            "    Uninstalling pydantic_core-2.41.5:\n",
            "      Successfully uninstalled pydantic_core-2.41.5\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1_modules 0.4.2\n",
            "    Uninstalling pyasn1_modules-0.4.2:\n",
            "      Successfully uninstalled pyasn1_modules-0.4.2\n",
            "  Attempting uninstall: marshmallow\n",
            "    Found existing installation: marshmallow 3.26.2\n",
            "    Uninstalling marshmallow-3.26.2:\n",
            "      Successfully uninstalled marshmallow-3.26.2\n",
            "  Attempting uninstall: jsonpatch\n",
            "    Found existing installation: jsonpatch 1.33\n",
            "    Uninstalling jsonpatch-1.33:\n",
            "      Successfully uninstalled jsonpatch-1.33\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: faiss-cpu\n",
            "    Found existing installation: faiss-cpu 1.13.2\n",
            "    Uninstalling faiss-cpu-1.13.2:\n",
            "      Successfully uninstalled faiss-cpu-1.13.2\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 2.0.0\n",
            "    Uninstalling cffi-2.0.0:\n",
            "      Successfully uninstalled cffi-2.0.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.12.1\n",
            "    Uninstalling anyio-4.12.1:\n",
            "      Successfully uninstalled anyio-4.12.1\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.4.0\n",
            "    Uninstalling aiosignal-1.4.0:\n",
            "      Successfully uninstalled aiosignal-1.4.0\n",
            "  Attempting uninstall: youtube-transcript-api\n",
            "    Found existing installation: youtube-transcript-api 0.6.2\n",
            "    Uninstalling youtube-transcript-api-0.6.2:\n",
            "      Successfully uninstalled youtube-transcript-api-0.6.2\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.12.0\n",
            "    Uninstalling tiktoken-0.12.0:\n",
            "      Successfully uninstalled tiktoken-0.12.0\n",
            "  Attempting uninstall: requests-toolbelt\n",
            "    Found existing installation: requests-toolbelt 1.0.0\n",
            "    Uninstalling requests-toolbelt-1.0.0:\n",
            "      Successfully uninstalled requests-toolbelt-1.0.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.12.5\n",
            "    Uninstalling pydantic-2.12.5:\n",
            "      Successfully uninstalled pydantic-2.12.5\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: dataclasses-json\n",
            "    Found existing installation: dataclasses-json 0.6.7\n",
            "    Uninstalling dataclasses-json-0.6.7:\n",
            "      Successfully uninstalled dataclasses-json-0.6.7\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 46.0.5\n",
            "    Uninstalling cryptography-46.0.5:\n",
            "      Successfully uninstalled cryptography-46.0.5\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.13.3\n",
            "    Uninstalling aiohttp-3.13.3:\n",
            "      Successfully uninstalled aiohttp-3.13.3\n",
            "  Attempting uninstall: pydantic-settings\n",
            "    Found existing installation: pydantic-settings 2.13.1\n",
            "    Uninstalling pydantic-settings-2.13.1:\n",
            "      Successfully uninstalled pydantic-settings-2.13.1\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.7.5\n",
            "    Uninstalling langsmith-0.7.5:\n",
            "      Successfully uninstalled langsmith-0.7.5\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.48.0\n",
            "    Uninstalling google-auth-2.48.0:\n",
            "      Successfully uninstalled google-auth-2.48.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.14\n",
            "    Uninstalling langchain-core-1.2.14:\n",
            "      Successfully uninstalled langchain-core-1.2.14\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 1.1.1\n",
            "    Uninstalling langchain-text-splitters-1.1.1:\n",
            "      Successfully uninstalled langchain-text-splitters-1.1.1\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.64.0\n",
            "    Uninstalling google-genai-1.64.0:\n",
            "      Successfully uninstalled google-genai-1.64.0\n",
            "  Attempting uninstall: langchain-google-genai\n",
            "    Found existing installation: langchain-google-genai 4.2.1\n",
            "    Uninstalling langchain-google-genai-4.2.1:\n",
            "      Successfully uninstalled langchain-google-genai-4.2.1\n",
            "  Attempting uninstall: langchain-classic\n",
            "    Found existing installation: langchain-classic 1.0.1\n",
            "    Uninstalling langchain-classic-1.0.1:\n",
            "      Successfully uninstalled langchain-classic-1.0.1\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.4.1\n",
            "    Uninstalling langchain-community-0.4.1:\n",
            "      Successfully uninstalled langchain-community-0.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.47.0, but you have google-auth 2.48.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 46.0.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\n",
            "gradio 5.50.0 requires pydantic<=2.12.3,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.3 SQLAlchemy-2.0.46 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.12.1 attrs-25.4.0 certifi-2026.1.4 cffi-2.0.0 charset_normalizer-3.4.4 cryptography-46.0.5 dataclasses-json-0.6.7 defusedxml-0.7.1 distro-1.9.0 faiss-cpu-1.13.2 filetype-1.2.0 frozenlist-1.8.0 google-auth-2.48.0 google-genai-1.64.0 greenlet-3.3.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 idna-3.11 jsonpatch-1.33 jsonpointer-3.0.0 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.14 langchain-google-genai-4.2.1 langchain-text-splitters-1.1.1 langsmith-0.7.5 marshmallow-3.26.2 multidict-6.7.1 mypy-extensions-1.1.0 numpy-2.4.2 orjson-3.11.7 packaging-26.0 propcache-0.4.1 pyasn1-0.6.2 pyasn1-modules-0.4.2 pycparser-3.0 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-settings-2.13.1 python-dotenv-1.2.1 regex-2026.2.19 requests-2.32.5 requests-toolbelt-1.0.0 rsa-4.9.1 sniffio-1.3.1 tenacity-9.1.4 tiktoken-0.12.0 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 urllib3-2.6.3 uuid-utils-0.14.0 websockets-15.0.1 xxhash-3.6.0 yarl-1.22.0 youtube-transcript-api-1.2.4 zstandard-0.25.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_cffi_backend",
                  "_openssl",
                  "aiohappyeyeballs",
                  "aiohttp",
                  "aiosignal",
                  "annotated_types",
                  "anyio",
                  "attr",
                  "attrs",
                  "certifi",
                  "cryptography",
                  "defusedxml",
                  "distro",
                  "dotenv",
                  "filetype",
                  "frozenlist",
                  "google",
                  "h11",
                  "httpcore",
                  "httpx",
                  "httpx_sse",
                  "idna",
                  "jsonpatch",
                  "jsonpointer",
                  "langchain_community",
                  "langchain_core",
                  "langchain_google_genai",
                  "langchain_text_splitters",
                  "langsmith",
                  "multidict",
                  "numpy",
                  "orjson",
                  "packaging",
                  "propcache",
                  "pyasn1",
                  "pydantic",
                  "pydantic_settings",
                  "regex",
                  "requests",
                  "requests_toolbelt",
                  "rsa",
                  "sniffio",
                  "tenacity",
                  "tiktoken",
                  "tiktoken_ext",
                  "typing_inspection",
                  "urllib3",
                  "websockets",
                  "xxhash",
                  "yaml",
                  "yarl",
                  "youtube_transcript_api",
                  "zstandard"
                ]
              },
              "id": "b33853f8ead541e2bbf0472e2f503be1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube transcript\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "\n",
        "# Text Splitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Gemini Embeddings + Gemini Chat Model\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "\n",
        "# Vector Store (FAISS)\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Prompt Template\n",
        "from langchain_core.prompts import PromptTemplate\n"
      ],
      "metadata": {
        "id": "ufUh_iykpA66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Indexing"
      ],
      "metadata": {
        "id": "PYMuBA00q4Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y youtube-transcript-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJFgnE3Y2f6o",
        "outputId": "06d29d74-30be-469d-e97c-f5525816d3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: youtube-transcript-api 1.2.4\n",
            "Uninstalling youtube-transcript-api-1.2.4:\n",
            "  Successfully uninstalled youtube-transcript-api-1.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api==0.6.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu5WqI_424rb",
        "outputId": "cf007e85-9256-46e2-f945-fc6a145986b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api==0.6.2\n",
            "  Using cached youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api==0.6.2) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api==0.6.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api==0.6.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api==0.6.2) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api==0.6.2) (2026.1.4)\n",
            "Using cached youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y youtube_transcript_api youtube-transcript-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6CojOIb3bFs",
        "outputId": "ae0aa41b-c20c-44d7-f3be-1fdc42fac0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping youtube-transcript-api as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api==0.6.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgCbPV3U3ect",
        "outputId": "8f89a2ce-1b6f-42a2-c4d2-b82175319292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api==0.6.2\n",
            "  Using cached youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api==0.6.2) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api==0.6.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api==0.6.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api==0.6.2) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api==0.6.2) (2026.1.4)\n",
            "Using cached youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade youtube-transcript-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "Jsz-YB4p60J8",
        "outputId": "e1a5ca56-0fdb-438e-fd15-9950637944e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Collecting youtube-transcript-api\n",
            "  Using cached youtube_transcript_api-1.2.4-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2026.1.4)\n",
            "Using cached youtube_transcript_api-1.2.4-py3-none-any.whl (485 kB)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "  Attempting uninstall: youtube-transcript-api\n",
            "    Found existing installation: youtube-transcript-api 0.6.2\n",
            "    Uninstalling youtube-transcript-api-0.6.2:\n",
            "      Successfully uninstalled youtube-transcript-api-0.6.2\n",
            "Successfully installed youtube-transcript-api-1.2.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "youtube_transcript_api"
                ]
              },
              "id": "71626bb026934008a542fbf2afdb2db6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "video_id = \"LPZh9BOjkQs\"\n",
        "\n",
        "try:\n",
        "    # Instance banao\n",
        "    api = YouTubeTranscriptApi()\n",
        "\n",
        "    # Direct fetch with preferred language\n",
        "    fetched_transcript = api.fetch(video_id, languages=['en'])  # ya ['en-US'], ['hi-IN'] etc.\n",
        "\n",
        "    # Ab .text attribute use karo har snippet ka\n",
        "    Transcript = \" \".join(snippet.text for snippet in fetched_transcript)\n",
        "\n",
        "    print(Transcript)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error bhai:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQATA7s1q23V",
        "outputId": "bd50a5dd-7248-4ff8-e03a-b464b145436b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, you lay out some text that describes an interaction between a user and a hypothetical AI assistant, add on whatever the user types in as the first part of the interaction, and then have the model repeatedly predict the next word that such a hypothetical AI assistant would say in response, and that's what's presented to the user. In doing this, the output tends to look a lot more natural if you allow it to select less likely words along the way at random. So what this means is even though the model itself is deterministic, a given prompt typically gives a different answer each time it's run. Models learn how to make these predictions by processing an enormous amount of text, typically pulled from the internet. For a standard human to read the amount of text that was used to train GPT-3, for example, if they read non-stop 24-7, it would take over 2600 years. Larger models since then train on much, much more. You can think of training a little bit like tuning the dials on a big machine. The way that a language model behaves is entirely determined by these many different continuous values, usually called parameters or weights. Changing those parameters will change the probabilities that the model gives for the next word on a given input. What puts the large in large language model is how they can have hundreds of billions of these parameters. No human ever deliberately sets those parameters. Instead, they begin at random, meaning the model just outputs gibberish, but they're repeatedly refined based on many example pieces of text. One of these training examples could be just a handful of words, or it could be thousands, but in either case, the way this works is to pass in all but the last word from that example into the model and compare the prediction that it makes with the true last word from the example. An algorithm called backpropagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true last word and a little less likely to choose all the others. When you do this for many, many trillions of examples, not only does the model start to give more accurate predictions on the training data, but it also starts to make more reasonable predictions on text that it's never seen before. Given the huge number of parameters and the enormous amount of training data, the scale of computation involved in training a large language model is mind-boggling. To illustrate, imagine that you could perform one billion additions and multiplications every single second. How long do you think it would take for you to do all of the operations involved in training the largest language models? Do you think it would take a year? Maybe something like 10,000 years? The answer is actually much more than that. It's well over 100 million years. This is only part of the story, though. This whole process is called pre-training. The goal of auto-completing a random passage of text from the internet is very different from the goal of being a good AI assistant. To address this, chatbots undergo another type of training, just as important, called reinforcement learning with human feedback. Workers flag unhelpful or problematic predictions, and their corrections further change the model's parameters, making them more likely to give predictions that users prefer. Looking back at the pre-training, though, this staggering amount of computation is only made possible by using special computer chips that are optimized for running many operations in parallel, known as GPUs. However, not all language models can be easily parallelized. Prior to 2017, most language models would process text one word at a time, but then a team of researchers at Google introduced a new model known as the transformer. Transformers don't read text from the start to the finish, they soak it all in at once, in parallel. The very first step inside a transformer, and most other language models for that matter, is to associate each word with a long list of numbers. The reason for this is that the training process only works with continuous values, so you have to somehow encode language using numbers, and each of these lists of numbers may somehow encode the meaning of the corresponding word. What makes transformers unique is their reliance on a special operation known as attention. This operation gives all of these lists of numbers a chance to talk to one another and refine the meanings they encode based on the context around, all done in parallel. For example, the numbers encoding the word bank might be changed based on the context surrounding it to somehow encode the more specific notion of a riverbank. Transformers typically also include a second type of operation known as a feed-forward neural network, and this gives the model extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental operations, and as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input text, as well as everything the model learned during training, to produce a prediction of the next word. Again, the model's prediction looks like a probability for every possible next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does. What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I make as a casual talk rather than a produced video, but I leave it up to you which one of these feels like the better follow-on.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Transcript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "uMh5WY-j8sJP",
        "outputId": "02731972-2d3e-4026-b1df-5ac5098f2c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, you lay out some text that describes an interaction between a user and a hypothetical AI assistant, add on whatever the user types in as the first part of the interaction, and then have the model repeatedly predict the next word that such a hypothetical AI assistant would say in response, and that's what's presented to the user. In doing this, the output tends to look a lot more natural if you allow it to select less likely words along the way at random. So what this means is even though the model itself is deterministic, a given prompt typically gives a different answer each time it's run. Models learn how to make these predictions by processing an enormous amount of text, typically pulled from the internet. For a standard human to read the amount of text that was used to train GPT-3, for example, if they read non-stop 24-7, it would take over 2600 years. Larger models since then train on much, much more. You can think of training a little bit like tuning the dials on a big machine. The way that a language model behaves is entirely determined by these many different continuous values, usually called parameters or weights. Changing those parameters will change the probabilities that the model gives for the next word on a given input. What puts the large in large language model is how they can have hundreds of billions of these parameters. No human ever deliberately sets those parameters. Instead, they begin at random, meaning the model just outputs gibberish, but they're repeatedly refined based on many example pieces of text. One of these training examples could be just a handful of words, or it could be thousands, but in either case, the way this works is to pass in all but the last word from that example into the model and compare the prediction that it makes with the true last word from the example. An algorithm called backpropagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true last word and a little less likely to choose all the others. When you do this for many, many trillions of examples, not only does the model start to give more accurate predictions on the training data, but it also starts to make more reasonable predictions on text that it's never seen before. Given the huge number of parameters and the enormous amount of training data, the scale of computation involved in training a large language model is mind-boggling. To illustrate, imagine that you could perform one billion additions and multiplications every single second. How long do you think it would take for you to do all of the operations involved in training the largest language models? Do you think it would take a year? Maybe something like 10,000 years? The answer is actually much more than that. It's well over 100 million years. This is only part of the story, though. This whole process is called pre-training. The goal of auto-completing a random passage of text from the internet is very different from the goal of being a good AI assistant. To address this, chatbots undergo another type of training, just as important, called reinforcement learning with human feedback. Workers flag unhelpful or problematic predictions, and their corrections further change the model's parameters, making them more likely to give predictions that users prefer. Looking back at the pre-training, though, this staggering amount of computation is only made possible by using special computer chips that are optimized for running many operations in parallel, known as GPUs. However, not all language models can be easily parallelized. Prior to 2017, most language models would process text one word at a time, but then a team of researchers at Google introduced a new model known as the transformer. Transformers don't read text from the start to the finish, they soak it all in at once, in parallel. The very first step inside a transformer, and most other language models for that matter, is to associate each word with a long list of numbers. The reason for this is that the training process only works with continuous values, so you have to somehow encode language using numbers, and each of these lists of numbers may somehow encode the meaning of the corresponding word. What makes transformers unique is their reliance on a special operation known as attention. This operation gives all of these lists of numbers a chance to talk to one another and refine the meanings they encode based on the context around, all done in parallel. For example, the numbers encoding the word bank might be changed based on the context surrounding it to somehow encode the more specific notion of a riverbank. Transformers typically also include a second type of operation known as a feed-forward neural network, and this gives the model extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental operations, and as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input text, as well as everything the model learned during training, to produce a prediction of the next word. Again, the model's prediction looks like a probability for every possible next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does. What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I make as a casual talk rather than a produced video, but I leave it up to you which one of these feels like the better follow-on.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### step - 2 : indexing-Text Splitting"
      ],
      "metadata": {
        "id": "T2LmDD8cCfRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "rA9NYhy_DtUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 500 , chunk_overlap = 50)\n",
        "chunks = splitter.create_documents([Transcript])\n",
        "documents = chunks\n",
        "\n",
        "## here chunk size 500 means every chunk\n",
        "## will holded 500 characters , and chunk overalp\n",
        "## means in every chunks 50 character will be repeating\n",
        "## in next chunks..to maintain and retain the context\n",
        "## of chunks."
      ],
      "metadata": {
        "id": "2vE_mJo_CfAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J83bJ_ZhI9_K",
        "outputId": "5ce3cae4-e01a-4a3a-b0be-169050128bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83RunjrcJGI1",
        "outputId": "86452472-a702-4bb4-a6b9-b32bbe3e2057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content=\"Imagine you happen across a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You could then finish the script by feeding in what you have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over\")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[16]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbm0fJWMJKU6",
        "outputId": "6cbd3b1f-8e9d-40c8-c497-30c66cc4179a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content='details of attention and all the other steps in a transformer. Also, on my second channel I just posted a talk I gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content I make as a casual talk rather than a produced video, but I leave it up to you which one of these feels like the better follow-on.')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##### Embedding Generation And Storing in Vector Store"
      ],
      "metadata": {
        "id": "pS3LS-AYLacX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass  # safe input ke liye\n",
        "\n",
        "# API key set karo (ek baar prompt karega, secure rahega)\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"AIzaSyBugOdvWRYnBAsh3cENrs2Tu4nuGzbSMKo\")\n",
        "\n",
        "# Ya direct testing ke liye (mat share karna kabhi)\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyYourNewValidKeyHere\""
      ],
      "metadata": {
        "id": "pAy01gn_T3_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "api_key = \"AIzaSyBugOdvWRYnBAsh3cENrs2Tu4nuGzbSMKo\"  # apna key daal\n",
        "url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent\"\n",
        "\n",
        "payload = {\n",
        "    \"contents\": [{\n",
        "        \"parts\": [{\"text\": \"Hello bhai, API key test: Haan ya Nahi?\"}]\n",
        "    }]\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"x-goog-api-key\": api_key\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"Key chal raha hai! Response:\", response.json()['candidates'][0]['content']['parts'][0]['text'])\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPKM3hnKWA-g",
        "outputId": "3b0a8bf2-2d88-4be7-927a-4ef0636197ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key chal raha hai! Response: Hello bhai! Achha sawaal hai! \n",
            "\n",
            "Mujhe personally kisi API key ki zaroorat nahi hoti ya main koi API key use nahi karta jo main 'test' kar sakoon ya jiska test kiya jaa sake, jaise ki ek software service karti hai. Main ek AI model hoon aur mujhse APIs ke through interact kiya jaata hai.\n",
            "\n",
            "Lekin, agar aap puchh rahe hain ki kya mujhe API keys ke baare mein jaankari hai aur main unki functionality, security, aur use cases samajhta hoon, to uska jawaab hai **'Haan!'**\n",
            "\n",
            "Toh, aapka irada kya tha is test se? Kis baare mein jaan'na chahte hain? Main aapki madad karne ke liye taiyaar hoon!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# API key set confirm\n",
        "print(\"Key set hai?\", \"GOOGLE_API_KEY\" in os.environ)\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
        "\n",
        "test_docs = [\"Hello bhai, ye test embedding hai.\"]\n",
        "try:\n",
        "    vectors = embeddings.embed_documents(test_docs)\n",
        "    print(\"Embedding length:\", len(vectors[0]))  # normally 768 ya 1536\n",
        "    print(\"Chal gaya bhai!\")\n",
        "except Exception as e:\n",
        "    print(\"Ab bhi error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJUhjYjtUGo6",
        "outputId": "5f7c3f7c-fde2-49e1-971f-7b927e5bcfbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key set hai? True\n",
            "Ab bhi error: Error embedding content (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\"   # ya \"models/text-embedding-004\" try kar sakta hai\n",
        ")\n",
        "\n",
        "test_embedding = embeddings.embed_query(\"Bhai ye test hai\")\n",
        "print(\"Embedding length:\", len(test_embedding))\n",
        "print(\"Success! Ab vector store bana sakta hai.\")  # ~768 ya 3072 dimensions aayega"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "sp-_P8PVLEyD",
        "outputId": "a786aa2d-265c-481f-fd41-68b5f90c6bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "GoogleGenerativeAIError",
          "evalue": "Error embedding content (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             result = self.client.models.embed_content(\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36membed_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5481\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5482\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_embed_content\u001b[0;34m(self, model, contents, content, embedding_api_type, config)\u001b[0m\n\u001b[1;32m   4481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4482\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   4483\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     )\n\u001b[0;32m-> 1396\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m     response_body = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1208\u001b[0m       )\n\u001b[0;32m-> 1209\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m       return HttpResponse(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3746737270.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bhai ye test hai\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embedding length:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Success! Ab vector store bana sakta hai.\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ~768 ya 3072 dimensions aayega\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Error embedding content ({e.status}): {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGoogleGenerativeAIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Error embedding content: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGoogleGenerativeAIError\u001b[0m: Error embedding content (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}"
          ]
        }
      ]
    }
  ]
}